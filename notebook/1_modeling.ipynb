{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from typing import Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import (\n",
    "    GlobalAveragePooling2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    ")\n",
    "from keras.models import Sequential, Model\n",
    "from keras.applications.efficientnet import EfficientNetB2\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    LearningRateScheduler,\n",
    "    EarlyStopping,\n",
    "    ReduceLROnPlateau,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path: str = \"../src\"\n",
    "sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.io import load_tf_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE: int = 260  # for EfficientNetB2\n",
    "EPOCHS: int = 10\n",
    "BATCH_SIZE: int = 16\n",
    "RANDOM_SEED: int = 8080\n",
    "DATA_ROOT: Path = Path(\"../data\")\n",
    "XRAY_IMAGES_ROOT: Path = Path(\"/home/uziel/Downloads/nih_chest_x_rays\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load samples and images metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_df = pd.read_csv(DATA_ROOT.joinpath(\"processed_annotations.csv\"))\n",
    "annot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_df[\"image_path\"] = annot_df[\"image_name\"].map(\n",
    "    {img_file.name: img_file for img_file in XRAY_IMAGES_ROOT.glob(\"**/*.png\")}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split data into training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_splits(\n",
    "    annot_df: pd.DataFrame,\n",
    "    stratify_col: str,\n",
    "    random_seed: int = 8080,\n",
    "    shuffle: bool = True,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Split samples into training and validation sets.\"\"\"\n",
    "    return train_test_split(\n",
    "        annot_df,\n",
    "        test_size=0.2,\n",
    "        random_state=random_seed,\n",
    "        shuffle=shuffle,\n",
    "        stratify=annot_df[stratify_col],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = create_splits(\n",
    "    annot_df=annot_df, stratify_col=\"pneumonia\", random_seed=RANDOM_SEED, shuffle=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check some key metadata distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        pd.concat(\n",
    "            [\n",
    "                train_data[col].value_counts(normalize=True).rename(col)\n",
    "                for col in [\"pneumonia\", \"patient_gender\", \"view_position\"]\n",
    "            ]\n",
    "        ).rename(\"train_data\"),\n",
    "        pd.concat(\n",
    "            [\n",
    "                val_data[col].value_counts(normalize=True).rename(col)\n",
    "                for col in [\"pneumonia\", \"patient_gender\", \"view_position\"]\n",
    "            ]\n",
    "        ).rename(\"val_data\"),\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All relevant metadata fields are equally distributed in training and validation sets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup image generators"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "\n",
    "Deprecated: tf.keras.preprocessing.image.ImageDataGenerator is not recommended for new code. Prefer loading images with tf.keras.utils.image_dataset_from_directory and transforming the output tf.data.Dataset with preprocessing layers. For more information, see the tutorials for loading images and augmenting images, as well as the preprocessing layer guide.\n",
    "\n",
    "Use tf.data approach to load images. See: https://www.tensorflow.org/tutorials/load_data/images and https://stackoverflow.com/questions/63636427/how-to-load-images-by-their-paths-in-dataframe-columns-for-dual-input-using-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_dataset(\n",
    "    data: pd.DataFrame,\n",
    "    batch_size: int = 32,\n",
    "    shuffle: bool = True,\n",
    "    random_seed: int = 8080,\n",
    "):\n",
    "    \"\"\"Create image dataset\n",
    "\n",
    "    Args:\n",
    "        train_data: Dataframe containing training data information.\n",
    "\n",
    "    Returns:\n",
    "        A dataset with image data\n",
    "    \"\"\"\n",
    "    # 1. Create images dataset\n",
    "    images = tf.data.Dataset.from_tensor_slices(\n",
    "        data[\"image_path\"].map(str).to_numpy()\n",
    "    ).map(load_tf_image)\n",
    "\n",
    "    # 2. Create labels dataset\n",
    "    def cast_label(label):\n",
    "        return tf.cast(label, tf.int32)\n",
    "\n",
    "    labels = tf.data.Dataset.from_tensor_slices(data[\"pneumonia\"].to_numpy()).map(\n",
    "        cast_label\n",
    "    )\n",
    "\n",
    "    # 3. Combine datasets\n",
    "    image_dataset = tf.data.Dataset.zip((images, labels)).batch(batch_size)\n",
    "\n",
    "    # 4. Shuffle data\n",
    "    if shuffle:\n",
    "        image_dataset.shuffle(1000, seed=random_seed)\n",
    "\n",
    "    return image_dataset\n",
    "\n",
    "\n",
    "def get_preprocessing_layers(img_size: int = 256):\n",
    "    \"\"\"Get pre-processing and image augmentation layers. Layers such as Resizing and\n",
    "    Rescaling are applied at both training and inference time, whereas the others are\n",
    "    only applied at training time.\n",
    "\n",
    "    Args:\n",
    "        img_size: Rescale images to this size.\n",
    "\n",
    "    Returns:\n",
    "        A Sequential object with all pre-processing layers.\n",
    "    \"\"\"\n",
    "    return tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Resizing(img_size, img_size),\n",
    "            tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "            tf.keras.layers.RandomTranslation(\n",
    "                height_factor=(-0.05, 0.05), width_factor=(-0.05, 0.05)\n",
    "            ),\n",
    "            tf.keras.layers.RandomZoom(\n",
    "                height_factor=(-0.05, 0.05), width_factor=(-0.05, 0.05)\n",
    "            ),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_image_dataset(\n",
    "    data=train_data, batch_size=BATCH_SIZE, shuffle=True, random_seed=RANDOM_SEED\n",
    ")\n",
    "val_dataset = get_image_dataset(\n",
    "    data=val_data, batch_size=128, shuffle=False, random_seed=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data augmentations on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_layers = get_preprocessing_layers(img_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_preprocessing(img, label):\n",
    "    return preprocessing_layers(img), label\n",
    "\n",
    "\n",
    "t_x, t_y = next(iter(train_dataset.map(apply_preprocessing)))\n",
    "fig, m_axs = plt.subplots(4, 4, figsize=(16, 16))\n",
    "for c_x, c_y, c_ax in zip(t_x, t_y, m_axs.flatten()):\n",
    "    preprocessing_layers\n",
    "    c_ax.imshow(c_x[:, :, 0], cmap=\"bone\")\n",
    "    if c_y == 1:\n",
    "        c_ax.set_title(\"Pneumonia\")\n",
    "    else:\n",
    "        c_ax.set_title(\"No Pneumonia\")\n",
    "    c_ax.axis(\"off\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful source: https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model():\n",
    "    effnet_model = EfficientNetB2(include_top=True, weights=\"imagenet\")\n",
    "    return Model(\n",
    "        inputs=effnet_model.input, outputs=effnet_model.get_layer(\"block7b_add\").output\n",
    "    )\n",
    "\n",
    "\n",
    "def build_model(base_model, preprocessing_layers):\n",
    "    # 1. Freeze all EfficientNet blocks except the last one (Block 7)\n",
    "    for layer in base_model.layers[:-28]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # 2. Build final model by adding some extra layers\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            preprocessing_layers,\n",
    "            base_model,\n",
    "            Flatten(),\n",
    "            Dense(1, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 3. Define optimizer, loss and metric to monitor\n",
    "    optimizer = Adam()\n",
    "    loss = \"binary_crossentropy\"\n",
    "    metrics = [\"binary_accuracy\"]\n",
    "\n",
    "    # 4. Compile model\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(load_pretrained_model(), preprocessing_layers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, val_dataset, epochs):\n",
    "    \"\"\"Train model\"\"\"\n",
    "    weight_path = \"{}_my_model.best.hdf5\".format(\"xray_class\")\n",
    "\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        weight_path,\n",
    "        monitor=\"binary_accuracy\",\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode=\"max\",\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    early = EarlyStopping(monitor=\"binary_accuracy\", mode=\"max\", patience=10)\n",
    "\n",
    "    callbacks_list = [checkpoint, early]\n",
    "\n",
    "    return model.fit(\n",
    "        train_dataset,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_dataset,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks_list,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_hist = train_model(model, train_dataset, val_dataset, epochs=EPOCHS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can begin our model-building & training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After training for some time, look at the performance of your model by plotting some performance statistics:\n",
    "\n",
    "Note, these figures will come in handy for your FDA documentation later in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## After training, make some predictions to assess your model's overall performance\n",
    "## Note that detecting pneumonia is hard even for trained expert radiologists,\n",
    "## so there is no need to make the model perfect.\n",
    "my_model.load_weights(weight_path)\n",
    "pred_Y = new_model.predict(valX, batch_size=32, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auc(t_y, p_y):\n",
    "    ## Hint: can use scikit-learn's built in functions here like roc_curve\n",
    "\n",
    "    # Todo\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "## what other performance statistics do you want to include here besides AUC?\n",
    "\n",
    "\n",
    "# def ...\n",
    "# Todo\n",
    "\n",
    "# def ...\n",
    "# Todo\n",
    "\n",
    "# Also consider plotting the history of your model training:\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    # Todo\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot figures\n",
    "\n",
    "# Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you feel you are done training, you'll need to decide the proper classification threshold that optimizes your model's performance for a given metric (e.g. accuracy, F1, precision, etc.  You decide) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the threshold that optimize your model's performance,\n",
    "## and use that threshold to make binary classification. Make sure you take all your metrics into consideration.\n",
    "\n",
    "# Todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's look at some examples of predicted v. true with our best model:\n",
    "\n",
    "# Todo\n",
    "\n",
    "# fig, m_axs = plt.subplots(10, 10, figsize = (16, 16))\n",
    "# i = 0\n",
    "# for (c_x, c_y, c_ax) in zip(valX[0:100], testY[0:100], m_axs.flatten()):\n",
    "#     c_ax.imshow(c_x[:,:,0], cmap = 'bone')\n",
    "#     if c_y == 1:\n",
    "#         if pred_Y[i] > YOUR_THRESHOLD:\n",
    "#             c_ax.set_title('1, 1')\n",
    "#         else:\n",
    "#             c_ax.set_title('1, 0')\n",
    "#     else:\n",
    "#         if pred_Y[i] > YOUR_THRESHOLD:\n",
    "#             c_ax.set_title('0, 1')\n",
    "#         else:\n",
    "#             c_ax.set_title('0, 0')\n",
    "#     c_ax.axis('off')\n",
    "#     i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Just save model architecture to a .json:\n",
    "\n",
    "model_json = my_model.to_json()\n",
    "with open(\"my_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penumonia_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e5e89c2dba0891c091c7b7448c14a29b5352f8c3493df3e31a7175706b270a92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
