{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, Optional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    balanced_accuracy_score,\n",
    "    precision_recall_curve,\n",
    "    PrecisionRecallDisplay,\n",
    ")\n",
    "from itertools import cycle\n",
    "from keras.applications.efficientnet import EfficientNetB2\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path: str = \"../src\"\n",
    "sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.io import load_tf_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optinally randomly sample this amount of images for training\n",
    "SAMPLE_SIZE: Optional[int] = 5000\n",
    "IMG_SIZE: int = 260  # for EfficientNetB2\n",
    "EPOCHS: int = 10\n",
    "BATCH_SIZE: int = 8\n",
    "RANDOM_SEED: int = 8080\n",
    "DATA_ROOT: Path = Path(\"../data\")\n",
    "OUTPUTS_DIR: Path = DATA_ROOT.joinpath(\"model_outputs\")\n",
    "XRAY_IMAGES_ROOT: Path = Path(\"/home/uziel/Downloads/nih_chest_x_rays\")\n",
    "CHECKPOINT_PATH: Path = OUTPUTS_DIR.joinpath(\"model_checkpoint\")\n",
    "HISTORY_PATH: Path = OUTPUTS_DIR.joinpath(\"training_history.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load samples and images metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_df = pd.read_csv(DATA_ROOT.joinpath(\"processed_annotations.csv\"))\n",
    "annot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_df[\"image_path\"] = annot_df[\"image_name\"].map(\n",
    "    {img_file.name: img_file for img_file in XRAY_IMAGES_ROOT.glob(\"**/*.png\")}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split data into training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_splits(\n",
    "    annot_df: pd.DataFrame,\n",
    "    stratify_col: str,\n",
    "    random_seed: int = 8080,\n",
    "    shuffle: bool = True,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Split samples into training and validation sets.\"\"\"\n",
    "    return train_test_split(\n",
    "        annot_df,\n",
    "        test_size=0.2,\n",
    "        random_state=random_seed,\n",
    "        shuffle=shuffle,\n",
    "        stratify=annot_df[stratify_col],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAMPLE_SIZE is not None:\n",
    "    annot_df = annot_df.sample(frac=1)[:SAMPLE_SIZE]\n",
    "\n",
    "train_data, val_data = create_splits(\n",
    "    annot_df=annot_df, stratify_col=\"pneumonia\", random_seed=RANDOM_SEED, shuffle=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check some key metadata distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        pd.concat(\n",
    "            [\n",
    "                train_data[col].value_counts(normalize=True).rename(col)\n",
    "                for col in [\"pneumonia\", \"patient_gender\", \"view_position\"]\n",
    "            ]\n",
    "        ).rename(\"train_data\"),\n",
    "        pd.concat(\n",
    "            [\n",
    "                val_data[col].value_counts(normalize=True).rename(col)\n",
    "                for col in [\"pneumonia\", \"patient_gender\", \"view_position\"]\n",
    "            ]\n",
    "        ).rename(\"val_data\"),\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All relevant metadata fields are equally distributed in training and validation sets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup image generators"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "\n",
    "Deprecated: tf.keras.preprocessing.image.ImageDataGenerator is not recommended for new code. Prefer loading images with tf.keras.utils.image_dataset_from_directory and transforming the output tf.data.Dataset with preprocessing layers. For more information, see the tutorials for loading images and augmenting images, as well as the preprocessing layer guide.\n",
    "\n",
    "Use tf.data approach to load images. See: https://www.tensorflow.org/tutorials/load_data/images and https://stackoverflow.com/questions/63636427/how-to-load-images-by-their-paths-in-dataframe-columns-for-dual-input-using-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_dataset(\n",
    "    data: pd.DataFrame,\n",
    "    batch_size: int = 32,\n",
    "    shuffle: bool = True,\n",
    "    random_seed: int = 8080,\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"Create image dataset\n",
    "\n",
    "    Args:\n",
    "        train_data: Dataframe containing training data information.\n",
    "\n",
    "    Returns:\n",
    "        A dataset with image data\n",
    "    \"\"\"\n",
    "    # 1. Create images dataset\n",
    "    images = tf.data.Dataset.from_tensor_slices(\n",
    "        data[\"image_path\"].map(str).to_numpy()\n",
    "    ).map(load_tf_image)\n",
    "\n",
    "    # 2. Create labels dataset\n",
    "    def cast_label(label):\n",
    "        return tf.cast(label, tf.int32)\n",
    "\n",
    "    labels = tf.data.Dataset.from_tensor_slices(data[\"pneumonia\"].to_numpy()).map(\n",
    "        cast_label\n",
    "    )\n",
    "\n",
    "    # 3. Combine datasets\n",
    "    image_dataset = tf.data.Dataset.zip((images, labels)).batch(batch_size)\n",
    "\n",
    "    # 4. Shuffle data\n",
    "    if shuffle:\n",
    "        image_dataset.shuffle(1000, seed=random_seed)\n",
    "\n",
    "    return image_dataset\n",
    "\n",
    "\n",
    "def get_preprocessing_layers(img_size: int = 256) -> tf.keras.Sequential:\n",
    "    \"\"\"Get pre-processing and image augmentation layers. Layers such as Resizing and\n",
    "    Rescaling are applied at both training and inference time, whereas the others are\n",
    "    only applied at training time.\n",
    "\n",
    "    Args:\n",
    "        img_size: Rescale images to this size.\n",
    "\n",
    "    Returns:\n",
    "        A Sequential object with all pre-processing layers.\n",
    "    \"\"\"\n",
    "    return tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Resizing(img_size, img_size),\n",
    "            tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "            tf.keras.layers.RandomTranslation(\n",
    "                height_factor=(-0.05, 0.05), width_factor=(-0.05, 0.05)\n",
    "            ),\n",
    "            tf.keras.layers.RandomZoom(\n",
    "                height_factor=(-0.05, 0.05), width_factor=(-0.05, 0.05)\n",
    "            ),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_image_dataset(\n",
    "    data=train_data, batch_size=BATCH_SIZE, shuffle=True, random_seed=RANDOM_SEED\n",
    ")\n",
    "val_dataset = get_image_dataset(\n",
    "    data=val_data, batch_size=128, shuffle=False, random_seed=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data augmentations on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_layers = get_preprocessing_layers(img_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_preprocessing(img, label):\n",
    "    return preprocessing_layers(img), label\n",
    "\n",
    "\n",
    "t_x, t_y = next(iter(train_dataset.map(apply_preprocessing)))\n",
    "fig, m_axs = plt.subplots(4, 4, figsize=(16, 16))\n",
    "for c_x, c_y, c_ax in zip(t_x, t_y, m_axs.flatten()):\n",
    "    preprocessing_layers\n",
    "    c_ax.imshow(c_x[:, :, 0], cmap=\"bone\")\n",
    "    if c_y == 1:\n",
    "        c_ax.set_title(\"Pneumonia\")\n",
    "    else:\n",
    "        c_ax.set_title(\"No Pneumonia\")\n",
    "    c_ax.axis(\"off\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful source: https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model(*args, **kwargs) -> tf.keras.Model:\n",
    "    effnet_model = EfficientNetB2(include_top=True, weights=\"imagenet\", *args, **kwargs)\n",
    "    return tf.keras.Model(\n",
    "        inputs=effnet_model.input, outputs=effnet_model.get_layer(\"block7b_add\").output\n",
    "    )\n",
    "\n",
    "\n",
    "def build_model(\n",
    "    base_model: tf.keras.Model, preprocessing_layers: tf.keras.Sequential\n",
    ") -> tf.keras.Sequential:\n",
    "    # 1. Freeze all EfficientNet blocks except the last one (Block 7)\n",
    "    for layer in base_model.layers[:-28]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # 2. Build final model by adding some extra layers\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            preprocessing_layers,\n",
    "            base_model,\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(1, activation=\"relu\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 3. Define optimizer, loss and metric to monitor\n",
    "    optimizer = Adam()\n",
    "    loss = \"binary_crossentropy\"\n",
    "    metrics = [\"binary_accuracy\"]\n",
    "\n",
    "    # 4. Compile model\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(load_pretrained_model(), preprocessing_layers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: tf.keras.Model,\n",
    "    train_dataset: tf.data.Dataset,\n",
    "    val_dataset: tf.data.Dataset,\n",
    "    checkpoint_path: Path,\n",
    "    epochs: int = 100,\n",
    "):\n",
    "    \"\"\"Train model\"\"\"\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_path,\n",
    "        monitor=\"binary_accuracy\",\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode=\"max\",\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    early = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"binary_accuracy\", mode=\"max\", patience=10\n",
    "    )\n",
    "\n",
    "    callbacks_list = [checkpoint, early]\n",
    "\n",
    "    return model.fit(\n",
    "        train_dataset,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_dataset,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks_list,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_hist = train_model(\n",
    "    model, train_dataset, val_dataset, checkpoint_path=CHECKPOINT_PATH, epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(training_hist.history).to_csv(HISTORY_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_metrics(y_true: tf.Tensor, y_pred: tf.Tensor) -> Dict[str, float]:\n",
    "    \"\"\"Compute multiple performance metrics\n",
    "\n",
    "    Args:\n",
    "        y_true: Ground truth labels for each observation.\n",
    "        y_pred: Predicted labels for each observation.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing multiple performance metrics\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"f1_score\": f1_score(y_true, y_pred),\n",
    "        \"roc_auc_score\": roc_auc_score(y_true, y_pred),\n",
    "        \"average_precision_score\": average_precision_score(y_true, y_pred),\n",
    "        \"balanced_accuracy_score\": balanced_accuracy_score(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_pr_curve(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "    \"\"\"Plot precision-recall curve\n",
    "\n",
    "    Args:\n",
    "        performance_metrics: A dictionary of performance metrics including recall,\n",
    "            precision and average precision scores.\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "    labels = []\n",
    "    for f_score in f_scores:\n",
    "        x = np.linspace(0.01, 1)\n",
    "        y = f_score * x / (2 * x - f_score)\n",
    "        (l,) = plt.plot(x[y >= 0], y[y >= 0], color=\"gray\", alpha=0.2)\n",
    "        plt.annotate(\"f1={0:0.1f}\".format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "    display = PrecisionRecallDisplay.from_predictions(y_true, y_pred, ax=ax)\n",
    "\n",
    "    # add the legend for the iso-f1 curves\n",
    "    handles, labels = display.ax_.get_legend_handles_labels()\n",
    "    handles.extend([l])\n",
    "    labels.extend([\"iso-f1 curves\"])\n",
    "\n",
    "    # set the legend and the axes\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.legend(handles=handles, labels=labels, loc=\"best\")\n",
    "    ax.set_title(\"Precision-Recall curve\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(CHECKPOINT_PATH)\n",
    "\n",
    "y_true = np.concatenate([y for x, y in val_dataset], axis=0)\n",
    "y_scores = model.predict(val_dataset, batch_size=32, verbose=True)\n",
    "y_pred = (y_scores.flatten() > 0.5).astype(int)\n",
    "\n",
    "performance_metrics = get_performance_metrics(y_true, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Precision-recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(y_true, y_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Find threshold that optimizes recall (sensitivity or true positive rate)\n",
    "\n",
    "> When a high recall test returns a negative result, you can be confident that the result is truly negative since a high recall test has low false negatives. Recall does not take false positives into account though, so you may have high recall but are still labeling a lot of negative cases as positive. Because of this, high recall tests are good for things like screening studies, where you want to make sure someone _doesnâ€™t_ have a disease or worklist prioritization where you want to make sure that people _without_ the disease are being de-prioritized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "\n",
    "pr_stats = (\n",
    "    pd.DataFrame(\n",
    "        data=precision_recall_curve(y_true, y_scores),\n",
    "        index=[\"precision\", \"recall\", \"threshold\"],\n",
    "    )\n",
    "    .transpose()\n",
    "    .sort_values([\"recall\", \"precision\"], ascending=False)\n",
    ")\n",
    "pr_stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we want to maximize recall over precision, the best threshold is obtained by sorting our thresholds by recall, and then by precision if there is a tie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_th = pr_stats[\"threshold\"][0]\n",
    "print(f\"The best threshold found was: {best_th}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Visualize predicted vs true with the best threshold found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, m_axs = plt.subplots(10, 10, figsize=(16, 16))\n",
    "\n",
    "for i, (c_x, c_y, c_ax) in enumerate(zip(*next(iter(val_dataset)), m_axs.flatten())):\n",
    "    c_ax.imshow(c_x[:, :, 0], cmap=\"bone\")\n",
    "    if c_y == 1:\n",
    "        if y_scores[i] > best_th:\n",
    "            c_ax.set_title(\"1, 1\")\n",
    "        else:\n",
    "            c_ax.set_title(\"1, 0\")\n",
    "    else:\n",
    "        if y_scores[i] > best_th:\n",
    "            c_ax.set_title(\"0, 1\")\n",
    "        else:\n",
    "            c_ax.set_title(\"0, 0\")\n",
    "    c_ax.axis(\"off\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Persist model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(OUTPUTS_DIR.joinpath(\"pneumonia_xray_classifier\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penumonia_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e5e89c2dba0891c091c7b7448c14a29b5352f8c3493df3e31a7175706b270a92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
