{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, Optional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    balanced_accuracy_score,\n",
    "    precision_recall_curve,\n",
    "    PrecisionRecallDisplay,\n",
    ")\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path: str = \"../src\"\n",
    "sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_modules.pneumonia_data_module import PneumoniaDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optinally randomly sample this amount of images for training\n",
    "SAMPLE_SIZE: Optional[int] = 10000\n",
    "IMG_SIZE: int = 260  # for EfficientNetB2\n",
    "EPOCHS: int = 25\n",
    "BATCH_SIZE: int = 128\n",
    "RANDOM_SEED: int = 8080\n",
    "DATA_ROOT: Path = Path(\"../data\")\n",
    "OUTPUTS_DIR: Path = DATA_ROOT.joinpath(\"model_outputs\")\n",
    "XRAY_IMAGES_ROOT: Path = Path(\"/home/uziel/Downloads/nih_chest_x_rays\")\n",
    "CHECKPOINT_PATH: Path = OUTPUTS_DIR.joinpath(\"model_checkpoint\")\n",
    "MODEL_PATH: Path = OUTPUTS_DIR.joinpath(\"pneumonia_xray_classifier\")\n",
    "HISTORY_PATH: Path = OUTPUTS_DIR.joinpath(\"training_history.csv\")\n",
    "BEST_TH_PATH: Path = OUTPUTS_DIR.joinpath(\"best_th.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load samples and images metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_df = pd.read_csv(DATA_ROOT.joinpath(\"processed_annotations.csv\"))\n",
    "annot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_df[\"image_path\"] = annot_df[\"image_name\"].map(\n",
    "    {img_file.name: img_file for img_file in XRAY_IMAGES_ROOT.glob(\"**/*.png\")}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get train, val and test data loaders\n",
    "\n",
    "We instantiate a Pytorch Lightning data module that takes care of the following under the hood:\n",
    "\n",
    "1. Split data into train, val and test sets.\n",
    "2. Set pre-processing and data augmentation transforms.\n",
    "3. Initialize train, val and test datasets.\n",
    "\n",
    "The data module can be used to extract the relevant data loaders of each set as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAMPLE_SIZE is not None:\n",
    "    annot_df = annot_df.sample(frac=1)[:SAMPLE_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = PneumoniaDataModule(annot_df)\n",
    "data_module.setup(\"\")\n",
    "\n",
    "train_loader = data_module.train_dataloader()\n",
    "val_loader = data_module.val_dataloader()\n",
    "test_loader = data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = (train_loader.dataset.data, val_loader.dataset.data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check some key metadata distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        pd.concat(\n",
    "            [\n",
    "                train_data[col].value_counts(normalize=True).rename(col)\n",
    "                for col in [\"pneumonia\", \"patient_gender\", \"view_position\"]\n",
    "            ]\n",
    "        ).rename(\"train_data\"),\n",
    "        pd.concat(\n",
    "            [\n",
    "                val_data[col].value_counts(normalize=True).rename(col)\n",
    "                for col in [\"pneumonia\", \"patient_gender\", \"view_position\"]\n",
    "            ]\n",
    "        ).rename(\"val_data\"),\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All relevant metadata fields are mostly equally distributed in training and validation sets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data augmentations on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x, t_y = next(iter(train_loader))\n",
    "fig, m_axs = plt.subplots(4, 4, figsize=(16, 16))\n",
    "for c_x, c_y, c_ax in zip(t_x, t_y, m_axs.flatten()):\n",
    "    c_ax.imshow(c_x.permute(1, 2, 0), cmap=\"bone\")\n",
    "    if c_y == 1:\n",
    "        c_ax.set_title(\"Pneumonia\")\n",
    "    else:\n",
    "        c_ax.set_title(\"No Pneumonia\")\n",
    "    c_ax.axis(\"off\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful source: https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model(*args, **kwargs) -> tf.keras.Model:\n",
    "    effnet_model = EfficientNetB2(include_top=True, weights=\"imagenet\", *args, **kwargs)\n",
    "    return tf.keras.Model(\n",
    "        inputs=effnet_model.input, outputs=effnet_model.get_layer(\"block7b_add\").output\n",
    "    )\n",
    "\n",
    "\n",
    "def build_model(\n",
    "    base_model: tf.keras.Model, preprocessing_layers: tf.keras.Sequential\n",
    ") -> tf.keras.Sequential:\n",
    "    # 1. Freeze all EfficientNet blocks except the last one (Block 7)\n",
    "    for layer in base_model.layers[:-28]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # 2. Build final model by adding some extra layers\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            preprocessing_layers,\n",
    "            base_model,\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 3. Define optimizer, loss and metric to monitor\n",
    "    optimizer = Adam()\n",
    "    loss = \"binary_focal_crossentropy\"\n",
    "    metrics = [\n",
    "        tf.keras.metrics.TruePositives(name=\"tp\"),\n",
    "        tf.keras.metrics.FalsePositives(name=\"fp\"),\n",
    "        tf.keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "        tf.keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "        tf.keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "        tf.keras.metrics.Precision(name=\"precision\"),\n",
    "        tf.keras.metrics.Recall(name=\"recall\"),\n",
    "        tf.keras.metrics.AUC(name=\"auc\"),\n",
    "        tf.keras.metrics.AUC(name=\"prc\", curve=\"PR\"),\n",
    "    ]\n",
    "\n",
    "    # 4. Compile model\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(load_pretrained_model(), preprocessing_layers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: tf.keras.Model,\n",
    "    train_dataset: tf.data.Dataset,\n",
    "    val_dataset: tf.data.Dataset,\n",
    "    checkpoint_path: Path,\n",
    "    epochs: int = 100,\n",
    "):\n",
    "    \"\"\"Train model\"\"\"\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_path,\n",
    "        monitor=\"Recall\",\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode=\"max\",\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    early = tf.keras.callbacks.EarlyStopping(monitor=\"recall\", mode=\"max\", patience=10)\n",
    "\n",
    "    callbacks_list = [checkpoint, early]\n",
    "\n",
    "    return model.fit(\n",
    "        train_dataset,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_dataset,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks_list,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_hist = train_model(\n",
    "    model, train_dataset, val_dataset, checkpoint_path=CHECKPOINT_PATH, epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(training_hist.history)\n",
    "history_df.to_csv(HISTORY_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_metrics(y_true: tf.Tensor, y_pred: tf.Tensor) -> Dict[str, float]:\n",
    "    \"\"\"Compute multiple performance metrics\n",
    "\n",
    "    Args:\n",
    "        y_true: Ground truth labels for each observation.\n",
    "        y_pred: Predicted labels for each observation.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing multiple performance metrics\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"f1_score\": f1_score(y_true, y_pred),\n",
    "        \"roc_auc_score\": roc_auc_score(y_true, y_pred),\n",
    "        \"average_precision_score\": average_precision_score(y_true, y_pred),\n",
    "        \"balanced_accuracy_score\": balanced_accuracy_score(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_pr_curve(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "    \"\"\"Plot precision-recall curve\n",
    "\n",
    "    Args:\n",
    "        performance_metrics: A dictionary of performance metrics including recall,\n",
    "            precision and average precision scores.\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "    labels = []\n",
    "    for f_score in f_scores:\n",
    "        x = np.linspace(0.01, 1)\n",
    "        y = f_score * x / (2 * x - f_score)\n",
    "        (l,) = plt.plot(x[y >= 0], y[y >= 0], color=\"gray\", alpha=0.2)\n",
    "        plt.annotate(\"f1={0:0.1f}\".format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "    display = PrecisionRecallDisplay.from_predictions(y_true, y_pred, ax=ax)\n",
    "\n",
    "    # add the legend for the iso-f1 curves\n",
    "    handles, labels = display.ax_.get_legend_handles_labels()\n",
    "    handles.extend([l])\n",
    "    labels.extend([\"iso-f1 curves\"])\n",
    "\n",
    "    # set the legend and the axes\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.legend(handles=handles, labels=labels, loc=\"best\")\n",
    "    ax.set_title(\"Precision-Recall curve\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_history(history_df: pd.DataFrame) -> None:\n",
    "    \"\"\"Plot training metrics.\n",
    "\n",
    "    Args:\n",
    "        history_df: History dataframe containing scores for each epoch.\n",
    "    \"\"\"\n",
    "    # 0. Rename columns to more meaningful names for plot legends\n",
    "    history_df = history_df.rename(\n",
    "        columns={\n",
    "            \"loss\": \"Training loss\",\n",
    "            \"val_loss\": \"Validation loss\",\n",
    "            \"recall\": \"Training recall\",\n",
    "            \"val_recall\": \"Validation recall\",\n",
    "            \"precision\": \"Training precision\",\n",
    "            \"val_precision\": \"Validation precision\",\n",
    "        }\n",
    "    )\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "    plot_kwargs = dict(xlabel=\"Epoch\", ylabel=\"Score\")\n",
    "    history_df[[\"Training loss\", \"Validation loss\"]].plot(\n",
    "        ax=axes[0], title=\"Loss during training\", **plot_kwargs\n",
    "    )\n",
    "    history_df[\n",
    "        [\n",
    "            \"Training recall\",\n",
    "            \"Validation recall\",\n",
    "            \"Training precision\",\n",
    "            \"Validation precision\",\n",
    "        ]\n",
    "    ].plot(ax=axes[1], title=\"Performance during training\", **plot_kwargs)\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(CHECKPOINT_PATH)\n",
    "\n",
    "y_true = np.concatenate([y for x, y in val_dataset], axis=0)\n",
    "y_scores = model.predict(val_dataset, batch_size=32)\n",
    "y_pred = (y_scores.flatten() > 0.5).astype(int)\n",
    "\n",
    "performance_metrics = get_performance_metrics(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Precision-recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(y_true, y_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Find threshold that optimizes recall (sensitivity or true positive rate)\n",
    "\n",
    "> When a high recall test returns a negative result, you can be confident that the result is truly negative since a high recall test has low false negatives. Recall does not take false positives into account though, so you may have high recall but are still labeling a lot of negative cases as positive. Because of this, high recall tests are good for things like screening studies, where you want to make sure someone _doesnâ€™t_ have a disease or worklist prioritization where you want to make sure that people _without_ the disease are being de-prioritized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "\n",
    "pr_stats = (\n",
    "    pd.DataFrame(\n",
    "        data=precision_recall_curve(y_true, y_scores),\n",
    "        index=[\"precision\", \"recall\", \"threshold\"],\n",
    "    )\n",
    "    .transpose()\n",
    "    .sort_values([\"recall\", \"precision\"], ascending=False)\n",
    ")\n",
    "pr_stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we want to maximize recall over precision, the best threshold is obtained by sorting our thresholds by recall, and then by precision if there is a tie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_th = pr_stats[\"threshold\"][0]\n",
    "\n",
    "BEST_TH_PATH.write_text(str(best_th))\n",
    "\n",
    "print(f\"The best threshold found was: {best_th}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Visualize predicted vs true with the best threshold found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, m_axs = plt.subplots(10, 10, figsize=(16, 16))\n",
    "\n",
    "for i, (c_x, c_y, c_ax) in enumerate(zip(*next(iter(val_dataset)), m_axs.flatten())):\n",
    "    c_ax.imshow(c_x[:, :, 0], cmap=\"bone\")\n",
    "    if c_y == 1:\n",
    "        if y_scores[i] > best_th:\n",
    "            c_ax.set_title(\"1, 1\")\n",
    "        else:\n",
    "            c_ax.set_title(\"1, 0\")\n",
    "    else:\n",
    "        if y_scores[i] > best_th:\n",
    "            c_ax.set_title(\"0, 1\")\n",
    "        else:\n",
    "            c_ax.set_title(\"0, 0\")\n",
    "    c_ax.axis(\"off\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Persist model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penumonia_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e5e89c2dba0891c091c7b7448c14a29b5352f8c3493df3e31a7175706b270a92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
