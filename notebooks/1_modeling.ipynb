{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_lightning import LightningModule, Trainer, LightningDataModule\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from itertools import chain\n",
    "from torch import Tensor\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve,\n",
    "    PrecisionRecallDisplay,\n",
    ")\n",
    "from typing import Iterable\n",
    "from rich import traceback\n",
    "from torchvision.models import MobileNet_V3_Large_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path: str = \"../src\"\n",
    "sys.path.append(src_path)\n",
    "_ = traceback.install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_modules.pneumonia_data_module import PneumoniaDataModule\n",
    "from models.pneumonia_classifier import PneumoniaClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS: int = 50\n",
    "REQUIRED_TRANSFORMS = MobileNet_V3_Large_Weights.DEFAULT.transforms()\n",
    "BATCH_SIZE: int = 6\n",
    "RANDOM_SEED: int = 8080\n",
    "DATA_ROOT: Path = Path(\"../data\")\n",
    "OUTPUTS_DIR: Path = DATA_ROOT.joinpath(\"model_outputs\")\n",
    "OUTPUTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "XRAY_IMAGES_ROOT: Path = Path(\"/home/uziel/Downloads/nih_chest_x_rays\")\n",
    "LOGS_PATH: Path = OUTPUTS_DIR.joinpath(\"mobilenet_v3_large\")\n",
    "HISTORY_PATH: Path = OUTPUTS_DIR.joinpath(\"training_history.csv\")\n",
    "BEST_TH_PATH: Path = OUTPUTS_DIR.joinpath(\"best_th.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load samples and images metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_df = pd.read_csv(DATA_ROOT.joinpath(\"processed_annotations.csv\"))\n",
    "annot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_df[\"image_path\"] = annot_df[\"image_name\"].map(\n",
    "    {img_file.name: img_file for img_file in XRAY_IMAGES_ROOT.glob(\"**/*.png\")}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get train, val and test data loaders\n",
    "\n",
    "We instantiate a Pytorch Lightning data module that takes care of the following under the hood:\n",
    "\n",
    "1. Split data into train, val and test sets.\n",
    "2. Set pre-processing and data augmentation transforms.\n",
    "3. Initialize train, val and test datasets.\n",
    "\n",
    "The data module can be used to extract the relevant data loaders of each set as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = PneumoniaDataModule(\n",
    "    annot_df,\n",
    "    required_transforms=REQUIRED_TRANSFORMS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    balance_train=True,\n",
    "    random_seed=RANDOM_SEED,\n",
    ")\n",
    "data_module.setup(\"\")\n",
    "\n",
    "train_loader = data_module.train_dataloader()\n",
    "val_loader = data_module.val_dataloader()\n",
    "test_loader = data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = (train_loader.dataset.data, val_loader.dataset.data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check some key metadata distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        pd.concat(\n",
    "            [\n",
    "                train_data[col].value_counts(normalize=True).rename(col)\n",
    "                for col in [\"pneumonia\", \"patient_gender\", \"view_position\"]\n",
    "            ]\n",
    "        ).rename(\"train_data\"),\n",
    "        pd.concat(\n",
    "            [\n",
    "                val_data[col].value_counts(normalize=True).rename(col)\n",
    "                for col in [\"pneumonia\", \"patient_gender\", \"view_position\"]\n",
    "            ]\n",
    "        ).rename(\"val_data\"),\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All relevant metadata fields are mostly equally distributed in training and validation sets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data augmentations on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x, t_y = next(iter(train_loader))\n",
    "fig, m_axs = plt.subplots(2, 3, figsize=(12, 8))\n",
    "for c_x, c_y, c_ax in zip(t_x, t_y, m_axs.flatten()):\n",
    "    c_ax.imshow(c_x.permute(1, 2, 0), cmap=\"bone\")\n",
    "    if c_y == 1:\n",
    "        c_ax.set_title(\"Pneumonia\")\n",
    "    else:\n",
    "        c_ax.set_title(\"No Pneumonia\")\n",
    "    c_ax.axis(\"off\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PneumoniaClassifier()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: LightningModule,\n",
    "    train_loader: LightningDataModule,\n",
    "    val_loader: LightningDataModule,\n",
    "    test_loader: LightningDataModule,\n",
    "    logs_path: Path,\n",
    "    epochs: int = 100,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Train model\n",
    "\n",
    "    Args:\n",
    "        model: Model to train.\n",
    "        train_loader: Training data loader.\n",
    "        logs_path: Where to store\n",
    "        epochs:\n",
    "\n",
    "    Returns:\n",
    "        Trainer object.\n",
    "    \"\"\"\n",
    "    trainer = Trainer(\n",
    "        default_root_dir=logs_path,\n",
    "        callbacks=[EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"min\")],\n",
    "        max_epochs=epochs,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    test_results = trainer.test(model=model, dataloaders=test_loader, ckpt_path=\"best\")\n",
    "\n",
    "    return trainer, test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer, test_results = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    logs_path=LOGS_PATH,\n",
    "    epochs=EPOCHS,\n",
    "    accelerator=\"gpu\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_curve(y_true: Tensor, y_pred: Tensor):\n",
    "    \"\"\"Plot precision-recall curve\n",
    "\n",
    "    Args:\n",
    "        performance_metrics: A dictionary of performance metrics including recall,\n",
    "            precision and average precision scores.\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "    labels = []\n",
    "    for f_score in f_scores:\n",
    "        x = np.linspace(0.01, 1)\n",
    "        y = f_score * x / (2 * x - f_score)\n",
    "        (l,) = plt.plot(x[y >= 0], y[y >= 0], color=\"gray\", alpha=0.2)\n",
    "        plt.annotate(\"f1={0:0.1f}\".format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "    display = PrecisionRecallDisplay.from_predictions(y_true, y_pred, ax=ax)\n",
    "\n",
    "    # add the legend for the iso-f1 curves\n",
    "    handles, labels = display.ax_.get_legend_handles_labels()\n",
    "    handles.extend([l])\n",
    "    labels.extend([\"iso-f1 curves\"])\n",
    "\n",
    "    # set the legend and the axes\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.legend(handles=handles, labels=labels, loc=\"best\")\n",
    "    ax.set_title(\"Precision-Recall curve\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def smooth(scalars: Iterable[float], weight: float = 0.5) -> Iterable[float]:\n",
    "    last = scalars[0]\n",
    "    smoothed = list()\n",
    "    for point in scalars:\n",
    "        smoothed_val = last * weight + (1 - weight) * point\n",
    "        smoothed.append(smoothed_val)\n",
    "        last = smoothed_val\n",
    "\n",
    "    return smoothed\n",
    "\n",
    "\n",
    "def plot_metrics(metrics: pd.DataFrame) -> None:\n",
    "    \"\"\"Plot training metrics.\n",
    "\n",
    "    Args:\n",
    "        history_df: History dataframe containing scores for each epoch.\n",
    "    \"\"\"\n",
    "    # 0. Prune metrics\n",
    "    metrics = metrics[\n",
    "        metrics.columns[metrics.columns.str.contains(\"|\".join((\"train\", \"val\")))]\n",
    "    ]\n",
    "    train_cols = [c for c in metrics.columns if \"train\" in c]\n",
    "    metrics[train_cols] = metrics[train_cols].shift(-1)\n",
    "    metrics = metrics.dropna(how=\"all\")\n",
    "\n",
    "    # 1. Plot setup\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(10, 15))\n",
    "    plot_kwargs = dict(xlabel=\"Training step\", ylabel=\"Score\")\n",
    "\n",
    "    # 1.1. Losses plot\n",
    "    loss_cols = metrics.columns[metrics.columns.str.contains(\"loss\")]\n",
    "    metrics[loss_cols].apply(lambda x: smooth(x, 0.9)).plot(\n",
    "        ax=axes[0], title=\"Loss during training\", **plot_kwargs\n",
    "    ).legend(loc=\"upper right\")\n",
    "\n",
    "    # 1.2. Stats cols\n",
    "    stats_cols = metrics.columns[\n",
    "        metrics.columns.str.contains(\"|\".join((\"true\", \"false\")))\n",
    "    ]\n",
    "    metrics[stats_cols].apply(lambda x: smooth(x, 0.9)).plot(\n",
    "        ax=axes[1], title=\"Stats during training\", **plot_kwargs\n",
    "    ).legend(loc=\"upper right\")\n",
    "\n",
    "    # 1.3. Stats cols\n",
    "    binary_metrics_cols = metrics.columns.difference(loss_cols).difference(stats_cols)\n",
    "    metrics[binary_metrics_cols].apply(lambda x: smooth(x, 0.9)).plot(\n",
    "        ax=axes[2], title=\"Stats during training\", **plot_kwargs\n",
    "    ).legend(loc=\"upper right\")\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_file = sorted(\n",
    "    LOGS_PATH.glob(\"**/metrics.csv\"),\n",
    "    key=lambda file: file.stat().st_mtime,\n",
    "    reverse=True,\n",
    ")[0]\n",
    "metrics = pd.read_csv(metrics_file)\n",
    "plot_metrics(metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Precision-recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(model=model, dataloaders=test_loader, ckpt_path=\"best\")\n",
    "y_true = [int(x) for x in chain(*[targets for img, targets in iter(test_loader)])]\n",
    "y_scores = [float(x) for x in chain(*predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve(y_true, y_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Find threshold that optimizes recall (sensitivity or true positive rate)\n",
    "\n",
    "> When a high recall test returns a negative result, you can be confident that the result is truly negative since a high recall test has low false negatives. Recall does not take false positives into account though, so you may have high recall but are still labeling a lot of negative cases as positive. Because of this, high recall tests are good for things like screening studies, where you want to make sure someone _doesn’t_ have a disease or worklist prioritization where you want to make sure that people _without_ the disease are being de-prioritized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "\n",
    "pr_stats = (\n",
    "    pd.DataFrame(\n",
    "        data=precision_recall_curve(y_true, y_scores),\n",
    "        index=[\"precision\", \"recall\", \"threshold\"],\n",
    "    )\n",
    "    .transpose()\n",
    "    .sort_values([\"recall\", \"precision\"], ascending=False)\n",
    ")\n",
    "pr_stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we want to maximize recall over precision, the best threshold is obtained by sorting our thresholds by recall, and then by precision if there is a tie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_th = pr_stats[\"threshold\"].iloc[0]\n",
    "\n",
    "BEST_TH_PATH.write_text(str(best_th))\n",
    "\n",
    "print(f\"The best threshold found was: {best_th}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Visualize predicted vs true with the best threshold found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, m_axs = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "for i, (c_x, c_y, c_ax) in enumerate(zip(*next(iter(val_loader)), m_axs.flatten())):\n",
    "    c_ax.imshow(c_x.permute(1, 2, 0), cmap=\"bone\")\n",
    "    if c_y == 1:\n",
    "        if y_scores[i] > best_th:\n",
    "            c_ax.set_title(\"1, 1\")\n",
    "        else:\n",
    "            c_ax.set_title(\"1, 0\")\n",
    "    else:\n",
    "        if y_scores[i] > best_th:\n",
    "            c_ax.set_title(\"0, 1\")\n",
    "        else:\n",
    "            c_ax.set_title(\"0, 0\")\n",
    "    c_ax.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penumonia_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e5e89c2dba0891c091c7b7448c14a29b5352f8c3493df3e31a7175706b270a92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
